{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3774b785-5b9b-403d-9c0d-684c7ed28b0d",
   "metadata": {},
   "source": [
    "To explore the use of PySpark RDD operations and understand how the collect() function works by performing transformations and actions on a student dataset, including calculating average marks and filtering students based on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b9e69-ca6d-4dbb-91e7-80b58abd9c7f",
   "metadata": {},
   "source": [
    "Dataset Overview\n",
    "\n",
    "The dataset is a CSV file named students.csv\n",
    "\n",
    "It contains the following fields:\n",
    "\n",
    "    ID – Unique identifier of the student\n",
    "    \n",
    "    Name – Student’s name\n",
    "    \n",
    "    Age – Age of the student\n",
    "    \n",
    "    Gender – Gender (M/F)\n",
    "    \n",
    "    Math – Marks scored in Mathematics\n",
    "    \n",
    "    Science – Marks scored in Science\n",
    "    \n",
    "    English – Marks scored in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61512e88-a034-400f-be22-4561273d698e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-H4K3I3AN:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec50669-b486-4dc3-8687-920ad261f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file (assuming students.csv is in working directory)\n",
    "data = sc.textFile(\"C:\\\\Users\\\\user\\\\Downloads\\\\students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104e7649-0ca9-4f04-a152-12f5e39e597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove header\n",
    "header = data.first()\n",
    "rows = data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda631cf-9a83-4295-9792-23beb0c9271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split by comma\n",
    "split_rdd = rows.map(lambda line: line.split(\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c884852-7c64-41d7-8d9e-081299b51f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Dataset (first 10 rows) ===\n",
      "['1', 'Alice', '20', 'F', '66', '92', '44']\n",
      "['2', 'Bob', '20', 'M', '82', '52', '77']\n",
      "['3', 'Charlie', '22', 'F', '43', '57', '76']\n",
      "['4', 'David', '19', 'M', '95', '69', '46']\n",
      "['5', 'Eva', '19', 'F', '62', '44', '96']\n",
      "['6', 'Frank', '22', 'F', '70', '78', '94']\n",
      "['7', 'Grace', '24', 'F', '67', '66', '93']\n",
      "['8', 'Henry', '21', 'F', '53', '82', '60']\n",
      "['9', 'Ivy', '19', 'M', '64', '52', '46']\n",
      "['10', 'Jack', '19', 'F', '44', '59', '60']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Student Dataset (first 10 rows) ===\")\n",
    "for row in split_rdd.take(10): # you can change 10 → 20, 50 etc.\n",
    " print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b908e13f-30a3-415e-871d-7544365030af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert fields into structured format\n",
    "# (id, name, age, gender, math, science, english)\n",
    "students_rdd = split_rdd.map(lambda x: (int(x[0]), x[1], int(x[2]), x[3], int(x[4]), int(x[5]), int(x[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d385307a-330d-4371-bede-c10c52f96db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate average marks for each student\n",
    "avg_marks_rdd = students_rdd.map(lambda x: (x[1], (x[4] + x[5] + x[6]) / 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a891e5-f071-4bfb-aa73-7f4e1f8c689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter students who scored avg >= 75\n",
    "passed_rdd = avg_marks_rdd.filter(lambda x: x[1] >= 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb4f5e7-a20d-4cc9-9852-b6b54ba05cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Sort students by avg marks (descending)\n",
    "sorted_passed_rdd = passed_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e9bfa81-801e-4605-b1ce-9c60363b5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Collect results to driver\n",
    "results = sorted_passed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d2e504-541e-40b5-b3f6-4b11d33ce3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Students with Average >= 75 ===\n",
      "Name: Leo, Avg Marks: 88.00\n",
      "Name: Olivia, Avg Marks: 88.00\n",
      "Name: Rita, Avg Marks: 86.67\n",
      "Name: Kathy, Avg Marks: 81.67\n",
      "Name: George, Avg Marks: 81.67\n",
      "Name: Frank, Avg Marks: 80.67\n",
      "Name: Oscar, Avg Marks: 80.00\n",
      "Name: Uma, Avg Marks: 78.33\n",
      "Name: Kyle, Avg Marks: 78.33\n",
      "Name: Matt, Avg Marks: 78.33\n",
      "Name: Tina, Avg Marks: 76.00\n",
      "Name: Victor, Avg Marks: 75.67\n",
      "Name: Grace, Avg Marks: 75.33\n",
      "Name: Mona, Avg Marks: 75.00\n",
      "Name: Will, Avg Marks: 75.00\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"=== Students with Average >= 75 ===\")\n",
    "for student in results:\n",
    " print(f\"Name: {student[0]}, Avg Marks: {student[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68faee3-6dfe-4ddb-8836-0d5627040395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of students who passed: 15\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Some extra RDD operations for practice\n",
    "# (a) Count how many students passed\n",
    "count_passed = passed_rdd.count()\n",
    "print(\"\\nNumber of students who passed:\", count_passed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6779f-2d23-49fc-8eb1-92b1c9a1dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Find max average scorer\n",
    "topper = passed_rdd.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "print(\"Topper:\", topper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b981f-72ae-442c-b6f8-e4894574b328",
   "metadata": {},
   "source": [
    "The program successfully demonstrates the use of PySpark RDD operations:\n",
    "\n",
    "    Loading and cleaning data (removing header, splitting rows).\n",
    "    \n",
    "    Transformations such as map(), filter(), and sortBy() to compute average marks and filter students.\n",
    "    \n",
    "    Actions such as collect(), count(), take(), and reduce() to retrieve results to the driver.\n",
    "\n",
    "Key results:\n",
    "\n",
    "    15 students scored an average ≥ 75.\n",
    "    \n",
    "    Olivia was identified as the top scorer with an average of 88.0.\n",
    "    \n",
    "    The workflow illustrates how Spark can process structured datasets efficiently, showcasing its ability in large-scale analytics and real-time      computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
